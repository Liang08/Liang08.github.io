<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Statistical Learning Method Chapter 1</title>
      <link href="2020/12/27/statistical-learning-method-1/"/>
      <url>2020/12/27/statistical-learning-method-1/</url>
      
        <content type="html"><![CDATA[<h1 id="S-1-统计学习方法概论"><a href="#S-1-统计学习方法概论" class="headerlink" title="$\S 1$ 统计学习方法概论"></a>$\S 1$ 统计学习方法概论</h1><h2 id="统计学习"><a href="#统计学习" class="headerlink" title="统计学习"></a>统计学习</h2><p>统计学习(Statistical Learning)是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。统计学习也称为统计机器学习(Statistical Machine Learning)。</p><h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><h4 id="输入空间、特征空间与输出空间"><a href="#输入空间、特征空间与输出空间" class="headerlink" title="输入空间、特征空间与输出空间"></a>输入空间、特征空间与输出空间</h4><p>在监督学习中，将输入与输出所有可能性的集合分别称为<strong>输入空间(Input Space) 与输出空间(Output Space)</strong>。输入与输出空间可以是有限元素的集合，也可以是整个欧氏空间。</p><p>每个具体的输入时一个<strong>实例(Instance)</strong>，通常由<strong>特征向量(Feature Vector)</strong>表示。</p><h2 id="第1章例题及习题解答"><a href="#第1章例题及习题解答" class="headerlink" title="第1章例题及习题解答"></a>第1章例题及习题解答</h2><h3 id="例1-1实现"><a href="#例1-1实现" class="headerlink" title="例1.1实现"></a>例1.1实现</h3><p>用目标函数$y=sin2{\pi}x$, 加上一个正态分布的噪音干扰，用多项式去拟合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> scipy <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> leastsq</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">real_func</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (np.sin(<span class="number">2</span> * x * np.pi))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_func</span>(<span class="params">p, x</span>):</span></span><br><span class="line">    f = np.poly1d(p)</span><br><span class="line">    <span class="keyword">return</span> f(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">residuals</span>(<span class="params">p, x, y</span>):</span></span><br><span class="line">    ret = fit_func(p, x) - y</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#生成十个点</span></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment">#用于绘图</span></span><br><span class="line">x_points = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">1000</span>)</span><br><span class="line">y_0 = real_func(x)</span><br><span class="line"><span class="comment">#含噪音的结果</span></span><br><span class="line">y = [np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>) + y0 <span class="keyword">for</span> y0 <span class="keyword">in</span> y_0]</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成M次多项式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fitting</span>(<span class="params">M = <span class="number">0</span></span>):</span></span><br><span class="line">    p_init = np.random.rand(M + <span class="number">1</span>)</span><br><span class="line">    p_lsq = leastsq(residuals, p_init, args=(x, y))</span><br><span class="line">    print(<span class="string">&#x27;Fitting Parameters:&#x27;</span>, p_lsq[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可视化</span></span><br><span class="line">    plt.plot(x_points, real_func(x_points), label=<span class="string">&#x27;real&#x27;</span>)</span><br><span class="line">    plt.plot(x_points, fit_func(p_lsq[<span class="number">0</span>], x_points), label=<span class="string">&#x27;fitted curve&#x27;</span>)</span><br><span class="line">    plt.plot(x, y, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;noise&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">return</span> p_lsq</span><br></pre></td></tr></table></figure><p><strong>M = 0</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p_lsq_0 = fitting(M=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>Fitting Parameters: [-0.02758859]</code></pre><p><img src="output_6_1.png" alt="png"></p><p><strong>M = 1</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p_lsq_1 = fitting(M=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>Fitting Parameters: [-1.33119752  0.63801017]</code></pre><p><img src="output_8_1.png" alt="png"></p><p><strong>M = 3</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p_lsq_3 = fitting(M=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><pre><code>Fitting Parameters: [ 21.12873508 -31.44187437  10.44269382  -0.05514685]</code></pre><p><img src="output_10_1.png" alt="png"></p><p><strong>M= 9</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p_lsq_9 = fitting(M=<span class="number">9</span>)</span><br></pre></td></tr></table></figure><pre><code>Fitting Parameters: [-1.40054714e+04  6.29342044e+04 -1.18600256e+05  1.21638867e+05 -7.36916417e+04  2.67652169e+04 -5.61456611e+03  5.90187108e+02 -1.64814992e+01 -6.32808245e-02]</code></pre><p><img src="output_12_1.png" alt="png"></p><p>此时，虽然拟合的多项式能通过所有的点，但是显然已经<strong>过拟合</strong>。当发生过拟合时，训练误差会减小，但是测试误差会增大。</p><h3 id="Hoeffding不等式"><a href="#Hoeffding不等式" class="headerlink" title="Hoeffding不等式"></a>Hoeffding不等式</h3><p>Hoeffding（霍夫丁）不等式</p><p>令$X_1, X_2, \ldots, X_N$是独立随机变量，且$X_i \in {a_i, b_i}$;$\bar{X}$是$X_1, X_2, \ldots, X_N$的均值，则对于任意$t&gt;0$,下列不等式成立：</p><script type="math/tex; mode=display">P[\bar{X} - E(\bar{X}) \geqslant t] \leqslant \exp(- \frac{2N^2t^2}{\sum_{i=1}^N(b_i - a_i)^2})</script><script type="math/tex; mode=display">P[E(\bar{X}) - \bar{X} \geqslant t] \leqslant \exp(- \frac{2N^2t^2}{\sum_{i=1}^N(b_i - a_i)^2})</script><h3 id="习题1"><a href="#习题1" class="headerlink" title="习题1"></a>习题1</h3><h4 id="1-1-统计学习方法三要素"><a href="#1-1-统计学习方法三要素" class="headerlink" title="1.1 统计学习方法三要素"></a>1.1 统计学习方法三要素</h4><p>说明伯努利模型的极大似然估计以及贝叶斯估计中的统计学习方法三要素</p><p>伯努利模型是定义在取值为0或1的随机变量上的概率分布。假设观测到伯努利模型n次独立的数据生成结果，其中k次的结果为1，这时可以用极大似然估计或贝叶斯估计来估计结果为1的概率</p><ol><li><p>极大似然估计:</p><p>模型：$\mathcal{F} = {f | f_p(x) = p^x(1-p)^{(1-x)} }$</p><p>策略：似然函数的最大化</p><p>算法：</p><script type="math/tex; mode=display">\mathop{\arg\max}_{p} L(p)= \mathop{\arg\max}_{p} \binom{n}{k}p^k(1-p)^{(n-k)}</script></li></ol><p>   <strong>解法</strong>：</p><p>   伯努利模型的似然函数</p><script type="math/tex; mode=display">   L(p|x) = p(X = x | p) = \binom{n}{k}p^k(1-p)^{(n-k)}</script><p>   对上述式子两边求导，得：</p><script type="math/tex; mode=display">   0 = \binom{n}{k} (kp^{k-1}(1-p)^{(n-k)}-(n-k)p^{k}(1-p)^{(n-k-1)})</script><p>   解得$p = \frac{k}{n}$</p><ol><li><p>贝叶斯估计：</p><p>模型：$\mathcal{F} = {f | f_p(x) = p^x(1-p)^{(1-x)} }$</p><p>策略：求参数期望</p><p>算法： </p><script type="math/tex; mode=display">\begin{aligned}        E_\pi\big[p \big| y_1,\cdots,y_n\big]          & = {\int_0^1}p\pi (p|y_1,\cdots,y_n) dp \\          & = {\int_0^1} p\frac{f_D(y_1,\cdots,y_n|p)\pi(p)}{\int_{\Omega}f_D(y_1,\cdots,y_n|p)\pi(p)dp}dp \\          & = {\int_0^1}\frac{p^{k+1}(1-p)^{(n-k)}}{\int_0^1 p^k(1-p)^{(n-k)}dp}dp\end{aligned}</script></li></ol><h4 id="1-2-经验风险最小化"><a href="#1-2-经验风险最小化" class="headerlink" title="1.2 经验风险最小化"></a>1.2 经验风险最小化</h4><p>通过经验风险最小化推导极大似然估计：</p><p>证明模型是条件概率模型，当损失函数是对数损失函数时，经验风险最小化等价于极大似然估计</p><p><strong>证明</strong>：</p><p>设模型的条件概率分布为$P_{\theta} (Y|X)$,极大似然估计的似然函数为：</p><script type="math/tex; mode=display">L(\theta) = \prod_D P_{\theta}(Y|X)</script><p>对上述式子两边求对数，得：</p><script type="math/tex; mode=display">\ln L(\theta) = \sum_D \ln P_{\theta}(Y|X) \\ \mathop{\arg \max}_{\theta} \sum_D \ln P_{\theta}(Y|X) = \mathop{\arg \min}_{\theta} \sum_D (- \ln P_{\theta}(Y|X))</script><p>得证.</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
          <category> Machine Learning </category>
          
          <category> Statistical Learning Method </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
