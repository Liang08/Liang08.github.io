<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Liang&#39;s Blog</title>
  
  
  <link href="https://liang08.gitlab.io/atom.xml" rel="self"/>
  
  <link href="https://liang08.gitlab.io/"/>
  <updated>2021-01-10T14:13:22.363Z</updated>
  <id>https://liang08.gitlab.io/</id>
  
  <author>
    <name>Liang Xu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>JavaScript 01</title>
    <link href="https://liang08.gitlab.io/2021/01/10/JavaScript-01/"/>
    <id>https://liang08.gitlab.io/2021/01/10/JavaScript-01/</id>
    <published>2021-01-10T14:04:12.000Z</published>
    <updated>2021-01-10T14:13:22.363Z</updated>
    
    
    
    
    <category term="Programming Languages" scheme="https://liang08.gitlab.io/categories/Programming-Languages/"/>
    
    <category term="JavaScript" scheme="https://liang08.gitlab.io/categories/Programming-Languages/JavaScript/"/>
    
    
    <category term="JavaScript" scheme="https://liang08.gitlab.io/tags/JavaScript/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 01 —— Tensors</title>
    <link href="https://liang08.gitlab.io/2021/01/10/Pytorch-01/"/>
    <id>https://liang08.gitlab.io/2021/01/10/Pytorch-01/</id>
    <published>2021-01-10T10:27:39.000Z</published>
    <updated>2021-01-12T02:15:52.797Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tensors-张量"><a href="#Tensors-张量" class="headerlink" title="Tensors(张量)"></a>Tensors(张量)</h1><p>Tensor是一种特殊的数据结构，它与array和matrix类似。在Pytorch中，我们用tensor来储存一个模型的输入和输出。Tensor与numpy中的ndarrays类似，但是tensor能使用GPU等硬件设备来加速计算。在使用之前要引入以下库：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h1 id="Tensor初始化"><a href="#Tensor初始化" class="headerlink" title="Tensor初始化"></a>Tensor初始化</h1><p>tensor有多种初始化的方法：</p><h2 id="从基本数据类型出发"><a href="#从基本数据类型出发" class="headerlink" title="从基本数据类型出发"></a>从基本数据类型出发</h2><p>tensor可由基本数据类型初始化，tensor的数据类型可以自动推导</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br></pre></td></tr></table></figure><h2 id="从Numpy-array出发"><a href="#从Numpy-array出发" class="headerlink" title="从Numpy array出发"></a>从Numpy array出发</h2><p>Tensor可以由Numpy array 类型初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np_array = np.array(data)</span><br><span class="line">x_np = torch.from_numpy(np_array)</span><br></pre></td></tr></table></figure><h2 id="从另一个Tensor出发"><a href="#从另一个Tensor出发" class="headerlink" title="从另一个Tensor出发"></a>从另一个Tensor出发</h2><p>Tensor可以由另一个tensor构建，它保持了原Tensor的性质（shape,dtype），除非提供新值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_ones = torch.ones_like(x_data)</span><br><span class="line">print(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;x_ones&#125;</span> \n&quot;</span>)</span><br><span class="line">x_rand = torch.rand_like(x_data, dtype = torch.<span class="built_in">float</span>)</span><br><span class="line">print(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;x_rand&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure><p>Output：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Ones Tensor: </span><br><span class="line"> tensor([[1, 1],</span><br><span class="line">        [1, 1]]) </span><br><span class="line"></span><br><span class="line">Random Tensor: </span><br><span class="line"> tensor([[0.4990, 0.6255],</span><br><span class="line">        [0.4148, 0.0162]]) </span><br></pre></td></tr></table></figure><h2 id="随机数或常数"><a href="#随机数或常数" class="headerlink" title="随机数或常数"></a>随机数或常数</h2><p>有几个用来构建Tensor的常用函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">shape = (<span class="number">2</span>, <span class="number">3</span>)  <span class="comment">#shape是一个用来表示tensor各维数量的tuple</span></span><br><span class="line">rand_tensor = torch.rand(shape)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">zeros_tensor = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;rand_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;ones_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;Zeros Tensor: \n <span class="subst">&#123;zeros_tensor&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Random Tensor: </span><br><span class="line"> tensor([[0.2932, 0.9156, 0.9483],</span><br><span class="line">        [0.4570, 0.9828, 0.2313]]) </span><br><span class="line"></span><br><span class="line">Ones Tensor: </span><br><span class="line"> tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]]) </span><br><span class="line"></span><br><span class="line">Zeros Tensor: </span><br><span class="line"> tensor([[0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]])</span><br></pre></td></tr></table></figure><h1 id="Tensor属性"><a href="#Tensor属性" class="headerlink" title="Tensor属性"></a>Tensor属性</h1><p>一个Tensor有多个属性，它们描述了tensor的shape, dtype和储存设备（CPU, GPU等）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f&quot;Shape of tensor: <span class="subst">&#123;tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;Datatype of tensor: <span class="subst">&#123;tensor.dtype&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;Device tensor is stored on: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Shape of tensor: torch.Size([3, 4])</span><br><span class="line">Datatype of tensor: torch.float32</span><br><span class="line">Device tensor is stored on: cpu</span><br></pre></td></tr></table></figure><h1 id="Tensor操作"><a href="#Tensor操作" class="headerlink" title="Tensor操作"></a>Tensor操作</h1><p>Tensor可以执行多种命令，具体的命令列表<a href="https://pytorch.org/docs/stable/torch.html">见此网页</a>，下面列举一些常用的</p><h2 id="转移到GPU"><a href="#转移到GPU" class="headerlink" title="转移到GPU"></a>转移到GPU</h2><p>如果有GPU和cuda，可以将tensor转移到GPU上</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">  tensot = tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="索引和切片"><a href="#索引和切片" class="headerlink" title="索引和切片"></a>索引和切片</h2><p>Tensor具有和numpy类似的索引和切片方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.ones(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">tensor[:, <span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">print(tensor)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.]])</span><br></pre></td></tr></table></figure><h2 id="Tensor合并"><a href="#Tensor合并" class="headerlink" title="Tensor合并"></a>Tensor合并</h2><p>可以使用<code>torch.cat</code>函数来合并几个tensor</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1 = t1 = torch.cat([tensor, tensor, tensor], dim = <span class="number">1</span>)</span><br><span class="line">print(t1)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])</span><br></pre></td></tr></table></figure><h2 id="Tensor乘法"><a href="#Tensor乘法" class="headerlink" title="Tensor乘法"></a>Tensor乘法</h2><p>Tensor的乘法主要有两种，一种是Tensor中<strong>各元素直接相乘</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This computes the element-wise product</span></span><br><span class="line">print(<span class="string">f&quot;tensor.mul(tensor) \n <span class="subst">&#123;tensor.mul(tensor)&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="comment"># Alternative syntax:</span></span><br><span class="line">print(<span class="string">f&quot;tensor * tensor \n <span class="subst">&#123;tensor * tensor&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor.mul(tensor) </span><br><span class="line"> tensor([[1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.]]) </span><br><span class="line"></span><br><span class="line">tensor * tensor </span><br><span class="line"> tensor([[1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.]])</span><br></pre></td></tr></table></figure><p>另一种是Tensor的<strong>矩阵相乘</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f&quot;tensor.matmul(tensor.T) \n <span class="subst">&#123;tensor.matmul(tensor.T)&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="comment"># Alternative syntax:</span></span><br><span class="line">print(<span class="string">f&quot;tensor @ tensor.T \n <span class="subst">&#123;tensor @ tensor.T&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor.matmul(tensor.T) </span><br><span class="line"> tensor([[3., 3., 3., 3.],</span><br><span class="line">        [3., 3., 3., 3.],</span><br><span class="line">        [3., 3., 3., 3.],</span><br><span class="line">        [3., 3., 3., 3.]]) </span><br><span class="line"></span><br><span class="line">tensor @ tensor.T </span><br><span class="line"> tensor([[3., 3., 3., 3.],</span><br><span class="line">        [3., 3., 3., 3.],</span><br><span class="line">        [3., 3., 3., 3.],</span><br><span class="line">        [3., 3., 3., 3.]])</span><br></pre></td></tr></table></figure><h2 id="原地操作"><a href="#原地操作" class="headerlink" title="原地操作"></a>原地操作</h2><p>带有<code>_</code>后缀的操作是原地操作。比如<code>x = copy_(y)</code>，<code>x = t_()</code>会改变x的值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(tensor, <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">tensor.add_(<span class="number">2</span>)</span><br><span class="line">print(tensor)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.]]) </span><br><span class="line"></span><br><span class="line">tensor([[3., 2., 3., 3.],</span><br><span class="line">        [3., 2., 3., 3.],</span><br><span class="line">        [3., 2., 3., 3.],</span><br><span class="line">        [3., 2., 3., 3.]])</span><br></pre></td></tr></table></figure><h1 id="Numpy-和-Tensor-的联系"><a href="#Numpy-和-Tensor-的联系" class="headerlink" title="Numpy 和 Tensor 的联系"></a>Numpy 和 Tensor 的联系</h1><p>在CPU上的Tensor和Numpy array共享一块内存空间，因此改变一个会带来另一个的改变</p><h2 id="Tensor到Numpy-array"><a href="#Tensor到Numpy-array" class="headerlink" title="Tensor到Numpy array"></a>Tensor到Numpy array</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t = torch.ones(<span class="number">5</span>)</span><br><span class="line">print(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line">n = t.numpy()</span><br><span class="line">print(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([1., 1., 1., 1., 1.])</span><br><span class="line">n: [1. 1. 1. 1. 1.]</span><br></pre></td></tr></table></figure><h2 id="Numpy-array到Tensor"><a href="#Numpy-array到Tensor" class="headerlink" title="Numpy array到Tensor"></a>Numpy array到Tensor</h2><p>调用<code>from_numpy</code>函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n = np.ones(<span class="number">5</span>)</span><br><span class="line">t = torch.from_numpy(n)</span><br></pre></td></tr></table></figure><h2 id="两者的关联"><a href="#两者的关联" class="headerlink" title="两者的关联"></a>两者的关联</h2><p>改变Tensor或Numpy array中的任一个都会使得另一个发生改变</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">np.add(n, <span class="number">1</span>, out=n)</span><br><span class="line">print(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br><span class="line">t.add_(<span class="number">1</span>)</span><br><span class="line">print(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([2., 2., 2., 2., 2.])</span><br><span class="line">n: [2. 2. 2. 2. 2.]</span><br><span class="line">t: tensor([3., 3., 3., 3., 3.])</span><br><span class="line">n: [3. 3. 3. 3. 3.]</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">Tensor，中文叫张量，本质是一个多维数组，目的是能创造更高维度的矩阵、向量</summary>
    
    
    
    <category term="Programming Languages" scheme="https://liang08.gitlab.io/categories/Programming-Languages/"/>
    
    <category term="Python" scheme="https://liang08.gitlab.io/categories/Programming-Languages/Python/"/>
    
    <category term="Pytorch" scheme="https://liang08.gitlab.io/categories/Programming-Languages/Python/Pytorch/"/>
    
    
    <category term="Machine Learning" scheme="https://liang08.gitlab.io/tags/Machine-Learning/"/>
    
    <category term="Python" scheme="https://liang08.gitlab.io/tags/Python/"/>
    
    <category term="Pytorch" scheme="https://liang08.gitlab.io/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Statistical Learning Method Chapter 1</title>
    <link href="https://liang08.gitlab.io/2020/12/27/statistical-learning-method-1/"/>
    <id>https://liang08.gitlab.io/2020/12/27/statistical-learning-method-1/</id>
    <published>2020-12-27T04:08:17.000Z</published>
    <updated>2021-01-12T02:34:37.533Z</updated>
    
    <content type="html"><![CDATA[<h1 id="统计学习"><a href="#统计学习" class="headerlink" title="统计学习"></a>统计学习</h1><p>统计学习(Statistical Learning)是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。统计学习也称为统计机器学习(Statistical Machine Learning)。</p><h1 id="基本分类"><a href="#基本分类" class="headerlink" title="基本分类"></a>基本分类</h1><h2 id="监督学习-Supervised-Learning"><a href="#监督学习-Supervised-Learning" class="headerlink" title="监督学习(Supervised Learning)"></a>监督学习(Supervised Learning)</h2><h3 id="输入空间、特征空间与输出空间"><a href="#输入空间、特征空间与输出空间" class="headerlink" title="输入空间、特征空间与输出空间"></a>输入空间、特征空间与输出空间</h3><p>在监督学习中，将输入与输出所有可能性的集合分别称为<strong>输入空间(Input Space) 与输出空间(Output Space)</strong>。输入与输出空间可以是有限元素的集合，也可以是整个欧氏空间。</p><p>每个具体的输入时一个<strong>实例(Instance)</strong>，通常由<strong>特征向量(Feature Vector)</strong>表示。所有特征向量存在的空间称为<strong>特种空间(Feature Space)</strong>。特征空间的每一维对应一个特征。</p><p>在监督学习中，将输入与输出看做是定义在输入（特征）空间上的与输出空间上的随机变量的取值。输入输出变量用大写字母表示，变量的取值用小写字母表示。输入实例$x$的特征向量记作</p><script type="math/tex; mode=display">x = \left( x^{(1)}, x^{(2)}, \ldots , x^{(i)}, \ldots, x^{(n)} \right)^T</script>]]></content>
    
    
    <summary type="html">统计学习方法概论</summary>
    
    
    
    <category term="Notes" scheme="https://liang08.gitlab.io/categories/Notes/"/>
    
    <category term="Machine Learning" scheme="https://liang08.gitlab.io/categories/Notes/Machine-Learning/"/>
    
    <category term="Statistical Learning Method" scheme="https://liang08.gitlab.io/categories/Notes/Machine-Learning/Statistical-Learning-Method/"/>
    
    
    <category term="Machine Learning" scheme="https://liang08.gitlab.io/tags/Machine-Learning/"/>
    
  </entry>
  
</feed>
